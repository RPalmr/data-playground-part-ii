{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Goal**: Get a better understanding of ***Neural Network hyperparameters***\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ Open the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.06711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=false&regularizationRate_hide=false) again to learn more about Neural Networks. \n",
    "\n",
    "‚ùóÔ∏è Keep in mind that as the algorithm is stochastic, the results may differ from one run to another. For this reason, do not hesitate to re-run the algorithms multiple times to analyse the behavior of your Neural Networks and draw your conclusions accordingly.\n",
    "\n",
    "üïµüèª Let's explore the different items we have seen during the lecture:\n",
    "- **Batch Size**\n",
    "- **Regularization**\n",
    "- **Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Initial Question** ‚ùì Select the `circle dataset` (Classification). \n",
    "\n",
    "* Build a model with: \n",
    "    * one hidden layer with 3 neurons,\n",
    "    * a _learning rate_ equal to 0.03, \n",
    "    * and the _tanh_ activation function\n",
    "\n",
    "* Do not add any noise (=0).\n",
    "\n",
    "* Select a batch size of 30\n",
    "\n",
    "***Look at the convergence of the algorithm. Does it seem slow or fast?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.7235 - accuracy: 0.4844 - val_loss: 0.7068 - val_accuracy: 0.5125\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7171 - accuracy: 0.4875 - val_loss: 0.7025 - val_accuracy: 0.5125\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7117 - accuracy: 0.4875 - val_loss: 0.6995 - val_accuracy: 0.5188\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7077 - accuracy: 0.4891 - val_loss: 0.6972 - val_accuracy: 0.5125\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7044 - accuracy: 0.4859 - val_loss: 0.6948 - val_accuracy: 0.5188\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.4844 - val_loss: 0.6928 - val_accuracy: 0.5250\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6993 - accuracy: 0.4844 - val_loss: 0.6911 - val_accuracy: 0.5312\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6976 - accuracy: 0.4938 - val_loss: 0.6903 - val_accuracy: 0.5312\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6898 - val_accuracy: 0.5375\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6944 - accuracy: 0.4938 - val_loss: 0.6894 - val_accuracy: 0.5312\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 989us/step - loss: 0.6934 - accuracy: 0.5031 - val_loss: 0.6887 - val_accuracy: 0.5312\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.4922 - val_loss: 0.6880 - val_accuracy: 0.5312\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5063 - val_loss: 0.6867 - val_accuracy: 0.5437\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.5078 - val_loss: 0.6859 - val_accuracy: 0.5750\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 998us/step - loss: 0.6897 - accuracy: 0.5500 - val_loss: 0.6853 - val_accuracy: 0.5813\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 974us/step - loss: 0.6888 - accuracy: 0.5500 - val_loss: 0.6847 - val_accuracy: 0.6187\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.6884 - accuracy: 0.6422 - val_loss: 0.6845 - val_accuracy: 0.6438\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 979us/step - loss: 0.6875 - accuracy: 0.6828 - val_loss: 0.6848 - val_accuracy: 0.6062\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 978us/step - loss: 0.6870 - accuracy: 0.6187 - val_loss: 0.6846 - val_accuracy: 0.6125\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.6031 - val_loss: 0.6837 - val_accuracy: 0.7312\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 966us/step - loss: 0.6858 - accuracy: 0.7406 - val_loss: 0.6835 - val_accuracy: 0.7625\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 966us/step - loss: 0.6852 - accuracy: 0.6641 - val_loss: 0.6832 - val_accuracy: 0.8125\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 980us/step - loss: 0.6846 - accuracy: 0.8062 - val_loss: 0.6825 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 999us/step - loss: 0.6840 - accuracy: 0.7469 - val_loss: 0.6822 - val_accuracy: 0.7937\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 974us/step - loss: 0.6836 - accuracy: 0.7312 - val_loss: 0.6813 - val_accuracy: 0.7063\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6829 - accuracy: 0.6984 - val_loss: 0.6806 - val_accuracy: 0.6875\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 956us/step - loss: 0.6824 - accuracy: 0.6687 - val_loss: 0.6805 - val_accuracy: 0.7125\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 947us/step - loss: 0.6817 - accuracy: 0.7406 - val_loss: 0.6796 - val_accuracy: 0.7125\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.6810 - accuracy: 0.6938 - val_loss: 0.6787 - val_accuracy: 0.7125\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6804 - accuracy: 0.6719 - val_loss: 0.6783 - val_accuracy: 0.7250\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.6797 - accuracy: 0.6859 - val_loss: 0.6781 - val_accuracy: 0.7563\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6789 - accuracy: 0.7344 - val_loss: 0.6779 - val_accuracy: 0.8188\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.7703 - val_loss: 0.6772 - val_accuracy: 0.8062\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.7594 - val_loss: 0.6767 - val_accuracy: 0.7750\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.6769 - accuracy: 0.7484 - val_loss: 0.6759 - val_accuracy: 0.7812\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6760 - accuracy: 0.7766 - val_loss: 0.6749 - val_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6752 - accuracy: 0.7703 - val_loss: 0.6744 - val_accuracy: 0.7750\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6743 - accuracy: 0.7563 - val_loss: 0.6734 - val_accuracy: 0.7875\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.7563 - val_loss: 0.6722 - val_accuracy: 0.7812\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6726 - accuracy: 0.7797 - val_loss: 0.6709 - val_accuracy: 0.7812\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6717 - accuracy: 0.7656 - val_loss: 0.6701 - val_accuracy: 0.7875\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.7703 - val_loss: 0.6694 - val_accuracy: 0.7937\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6698 - accuracy: 0.7625 - val_loss: 0.6678 - val_accuracy: 0.7688\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.7625 - val_loss: 0.6667 - val_accuracy: 0.7750\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.7672 - val_loss: 0.6652 - val_accuracy: 0.7563\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6666 - accuracy: 0.7422 - val_loss: 0.6646 - val_accuracy: 0.7750\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.7578 - val_loss: 0.6631 - val_accuracy: 0.7625\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6645 - accuracy: 0.7516 - val_loss: 0.6625 - val_accuracy: 0.7937\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6633 - accuracy: 0.7625 - val_loss: 0.6616 - val_accuracy: 0.7937\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 945us/step - loss: 0.6621 - accuracy: 0.7750 - val_loss: 0.6604 - val_accuracy: 0.7937\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 963us/step - loss: 0.6609 - accuracy: 0.7719 - val_loss: 0.6587 - val_accuracy: 0.7937\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 959us/step - loss: 0.6596 - accuracy: 0.7609 - val_loss: 0.6575 - val_accuracy: 0.7937\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.6584 - accuracy: 0.7797 - val_loss: 0.6562 - val_accuracy: 0.7875\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 969us/step - loss: 0.6569 - accuracy: 0.7734 - val_loss: 0.6546 - val_accuracy: 0.7875\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 959us/step - loss: 0.6555 - accuracy: 0.7688 - val_loss: 0.6527 - val_accuracy: 0.7812\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 936us/step - loss: 0.6542 - accuracy: 0.7547 - val_loss: 0.6511 - val_accuracy: 0.7750\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.6528 - accuracy: 0.7531 - val_loss: 0.6498 - val_accuracy: 0.7875\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6513 - accuracy: 0.7641 - val_loss: 0.6483 - val_accuracy: 0.7937\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 938us/step - loss: 0.6498 - accuracy: 0.7688 - val_loss: 0.6467 - val_accuracy: 0.7875\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 955us/step - loss: 0.6484 - accuracy: 0.7719 - val_loss: 0.6453 - val_accuracy: 0.7875\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 934us/step - loss: 0.6467 - accuracy: 0.7625 - val_loss: 0.6438 - val_accuracy: 0.7875\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 941us/step - loss: 0.6452 - accuracy: 0.7859 - val_loss: 0.6424 - val_accuracy: 0.7937\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6436 - accuracy: 0.7781 - val_loss: 0.6409 - val_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.7922 - val_loss: 0.6390 - val_accuracy: 0.7875\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.6401 - accuracy: 0.7922 - val_loss: 0.6372 - val_accuracy: 0.7937\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.7812 - val_loss: 0.6354 - val_accuracy: 0.7937\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6367 - accuracy: 0.7906 - val_loss: 0.6335 - val_accuracy: 0.7937\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 986us/step - loss: 0.6348 - accuracy: 0.7906 - val_loss: 0.6324 - val_accuracy: 0.8125\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 952us/step - loss: 0.6332 - accuracy: 0.7969 - val_loss: 0.6305 - val_accuracy: 0.8062\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.6314 - accuracy: 0.8109 - val_loss: 0.6282 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.6296 - accuracy: 0.8000 - val_loss: 0.6257 - val_accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 936us/step - loss: 0.6276 - accuracy: 0.7922 - val_loss: 0.6238 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 970us/step - loss: 0.6256 - accuracy: 0.7906 - val_loss: 0.6221 - val_accuracy: 0.8062\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 931us/step - loss: 0.6238 - accuracy: 0.8000 - val_loss: 0.6205 - val_accuracy: 0.8062\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 952us/step - loss: 0.6220 - accuracy: 0.8047 - val_loss: 0.6185 - val_accuracy: 0.8062\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 945us/step - loss: 0.6199 - accuracy: 0.8031 - val_loss: 0.6162 - val_accuracy: 0.8062\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 931us/step - loss: 0.6179 - accuracy: 0.8078 - val_loss: 0.6133 - val_accuracy: 0.7937\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 926us/step - loss: 0.6162 - accuracy: 0.7906 - val_loss: 0.6116 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 932us/step - loss: 0.6138 - accuracy: 0.7922 - val_loss: 0.6098 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 929us/step - loss: 0.6120 - accuracy: 0.7953 - val_loss: 0.6082 - val_accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 932us/step - loss: 0.6099 - accuracy: 0.8031 - val_loss: 0.6064 - val_accuracy: 0.8062\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 943us/step - loss: 0.6079 - accuracy: 0.8078 - val_loss: 0.6035 - val_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 978us/step - loss: 0.6058 - accuracy: 0.8031 - val_loss: 0.6013 - val_accuracy: 0.7937\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 930us/step - loss: 0.6039 - accuracy: 0.8016 - val_loss: 0.5995 - val_accuracy: 0.8000\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 928us/step - loss: 0.6018 - accuracy: 0.8062 - val_loss: 0.5974 - val_accuracy: 0.8062\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 984us/step - loss: 0.5996 - accuracy: 0.8078 - val_loss: 0.5954 - val_accuracy: 0.8062\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 939us/step - loss: 0.5975 - accuracy: 0.8078 - val_loss: 0.5931 - val_accuracy: 0.8062\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 932us/step - loss: 0.5954 - accuracy: 0.8062 - val_loss: 0.5909 - val_accuracy: 0.8125\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 928us/step - loss: 0.5933 - accuracy: 0.8031 - val_loss: 0.5897 - val_accuracy: 0.8125\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 945us/step - loss: 0.5913 - accuracy: 0.8125 - val_loss: 0.5875 - val_accuracy: 0.8125\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 947us/step - loss: 0.5891 - accuracy: 0.8125 - val_loss: 0.5855 - val_accuracy: 0.8125\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5869 - accuracy: 0.8109 - val_loss: 0.5840 - val_accuracy: 0.8125\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5848 - accuracy: 0.8141 - val_loss: 0.5823 - val_accuracy: 0.8188\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.8141 - val_loss: 0.5797 - val_accuracy: 0.8188\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5807 - accuracy: 0.8141 - val_loss: 0.5777 - val_accuracy: 0.8125\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 955us/step - loss: 0.5784 - accuracy: 0.8172 - val_loss: 0.5750 - val_accuracy: 0.8125\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 929us/step - loss: 0.5761 - accuracy: 0.8172 - val_loss: 0.5728 - val_accuracy: 0.8125\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 931us/step - loss: 0.5741 - accuracy: 0.8188 - val_loss: 0.5707 - val_accuracy: 0.8125\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 943us/step - loss: 0.5716 - accuracy: 0.8141 - val_loss: 0.5682 - val_accuracy: 0.8125\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 978us/step - loss: 0.5694 - accuracy: 0.8203 - val_loss: 0.5659 - val_accuracy: 0.8125\n",
      "7/7 [==============================] - 0s 440us/step - loss: 0.5694 - accuracy: 0.8250\n",
      "Test Loss: 0.5694, Test Accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.0, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 100\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is happening with a batch size of 1** ‚ùì \n",
    "\n",
    "Now, run this neural network on the same dataset but... \n",
    "\n",
    "* with a batch-size of 1.\n",
    "* Make sure to run it for at least 150 epochs. \n",
    "\n",
    "***What do you notice about the train and test loss? What is the reason of this instability?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "640/640 [==============================] - 0s 537us/step - loss: 0.7000 - accuracy: 0.4891 - val_loss: 0.6961 - val_accuracy: 0.3812\n",
      "Epoch 2/150\n",
      "640/640 [==============================] - 0s 378us/step - loss: 0.6829 - accuracy: 0.5703 - val_loss: 0.6596 - val_accuracy: 0.6750\n",
      "Epoch 3/150\n",
      "640/640 [==============================] - 0s 368us/step - loss: 0.6529 - accuracy: 0.6953 - val_loss: 0.6157 - val_accuracy: 0.7812\n",
      "Epoch 4/150\n",
      "640/640 [==============================] - 0s 371us/step - loss: 0.6095 - accuracy: 0.7609 - val_loss: 0.5687 - val_accuracy: 0.8438\n",
      "Epoch 5/150\n",
      "640/640 [==============================] - 0s 369us/step - loss: 0.5696 - accuracy: 0.7734 - val_loss: 0.5400 - val_accuracy: 0.7625\n",
      "Epoch 6/150\n",
      "640/640 [==============================] - 0s 374us/step - loss: 0.5397 - accuracy: 0.7828 - val_loss: 0.5417 - val_accuracy: 0.6750\n",
      "Epoch 7/150\n",
      "640/640 [==============================] - 0s 379us/step - loss: 0.5086 - accuracy: 0.7859 - val_loss: 0.4719 - val_accuracy: 0.8062\n",
      "Epoch 8/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.4343 - accuracy: 0.8547 - val_loss: 0.3732 - val_accuracy: 0.8750\n",
      "Epoch 9/150\n",
      "640/640 [==============================] - 0s 378us/step - loss: 0.3251 - accuracy: 0.9656 - val_loss: 0.2782 - val_accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.2496 - accuracy: 1.0000 - val_loss: 0.2247 - val_accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "640/640 [==============================] - 0s 375us/step - loss: 0.2017 - accuracy: 1.0000 - val_loss: 0.1811 - val_accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "640/640 [==============================] - 0s 418us/step - loss: 0.1671 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.1431 - accuracy: 1.0000 - val_loss: 0.1342 - val_accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "640/640 [==============================] - 0s 371us/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 0.1159 - val_accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "640/640 [==============================] - 0s 384us/step - loss: 0.1099 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "640/640 [==============================] - 0s 369us/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 0.0936 - val_accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "640/640 [==============================] - 0s 374us/step - loss: 0.0891 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "640/640 [==============================] - 0s 381us/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "640/640 [==============================] - 0s 366us/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "640/640 [==============================] - 0s 373us/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "640/640 [==============================] - 0s 368us/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "640/640 [==============================] - 0s 412us/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "640/640 [==============================] - 0s 381us/step - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "640/640 [==============================] - 0s 381us/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "640/640 [==============================] - 0s 376us/step - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "640/640 [==============================] - 0s 374us/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "640/640 [==============================] - 0s 371us/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "640/640 [==============================] - 0s 420us/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "640/640 [==============================] - 0s 378us/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "640/640 [==============================] - 0s 368us/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "640/640 [==============================] - 0s 375us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "640/640 [==============================] - 0s 372us/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "640/640 [==============================] - 0s 377us/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "640/640 [==============================] - 0s 363us/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "640/640 [==============================] - 0s 362us/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "640/640 [==============================] - 0s 360us/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "640/640 [==============================] - 0s 363us/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "640/640 [==============================] - 0s 364us/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "640/640 [==============================] - 0s 407us/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "640/640 [==============================] - 0s 406us/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "640/640 [==============================] - 0s 389us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "640/640 [==============================] - 0s 398us/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "640/640 [==============================] - 0s 384us/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "640/640 [==============================] - 0s 415us/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "640/640 [==============================] - 0s 372us/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "640/640 [==============================] - 0s 361us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "640/640 [==============================] - 0s 377us/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "640/640 [==============================] - 0s 382us/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "640/640 [==============================] - 0s 418us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "640/640 [==============================] - 0s 379us/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "640/640 [==============================] - 0s 400us/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "640/640 [==============================] - 0s 386us/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "640/640 [==============================] - 0s 379us/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "640/640 [==============================] - 0s 387us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "640/640 [==============================] - 0s 380us/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "640/640 [==============================] - 0s 379us/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "640/640 [==============================] - 0s 408us/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "640/640 [==============================] - 0s 388us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "640/640 [==============================] - 0s 398us/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "640/640 [==============================] - 0s 386us/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "640/640 [==============================] - 0s 386us/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "640/640 [==============================] - 0s 387us/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "640/640 [==============================] - 0s 381us/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "640/640 [==============================] - 0s 374us/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "640/640 [==============================] - 0s 375us/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "640/640 [==============================] - 0s 364us/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "640/640 [==============================] - 0s 360us/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "640/640 [==============================] - 0s 360us/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "640/640 [==============================] - 0s 373us/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "640/640 [==============================] - 0s 365us/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "640/640 [==============================] - 0s 356us/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "640/640 [==============================] - 0s 363us/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "640/640 [==============================] - 0s 363us/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "640/640 [==============================] - 0s 384us/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "640/640 [==============================] - 0s 391us/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "640/640 [==============================] - 0s 356us/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "640/640 [==============================] - 0s 369us/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "640/640 [==============================] - 0s 362us/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "640/640 [==============================] - 0s 373us/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "640/640 [==============================] - 0s 361us/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "640/640 [==============================] - 0s 394us/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "640/640 [==============================] - 0s 368us/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "640/640 [==============================] - 0s 356us/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "640/640 [==============================] - 0s 366us/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "640/640 [==============================] - 0s 365us/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "640/640 [==============================] - 0s 356us/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "640/640 [==============================] - 0s 361us/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "640/640 [==============================] - 0s 385us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "640/640 [==============================] - 0s 363us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "640/640 [==============================] - 0s 383us/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "640/640 [==============================] - 0s 366us/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "640/640 [==============================] - 0s 357us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "640/640 [==============================] - 0s 362us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "640/640 [==============================] - 0s 358us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "640/640 [==============================] - 0s 359us/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 492us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Test Loss: 0.0080, Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.0, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 150\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question/Observation** ‚ùì \n",
    "\n",
    "Now, you can see the effect of the _batch_size_ by reading the values of the train loss and test loss: pause the iterations and run it step by step (iteration per iteration) using the `\"Step\"` button (at the right side of the play/stop button)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Answer here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the lack of generalization** ‚ùì \n",
    "\n",
    "To once again observe the **lack of generalization**:\n",
    "* Select the `\"eXclusive OR\"(XOR)` dataset, \n",
    "* with a noise of 50,\n",
    "* Add a second hidden layer with again 8 neurons. \n",
    "\n",
    "***Try to fit your model once again... what do you expect?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.7543 - accuracy: 0.5328 - val_loss: 0.7503 - val_accuracy: 0.5562\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7478 - accuracy: 0.5344 - val_loss: 0.7460 - val_accuracy: 0.5437\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7419 - accuracy: 0.5578 - val_loss: 0.7413 - val_accuracy: 0.5312\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7370 - accuracy: 0.5531 - val_loss: 0.7369 - val_accuracy: 0.5312\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7322 - accuracy: 0.5453 - val_loss: 0.7334 - val_accuracy: 0.5375\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7278 - accuracy: 0.5703 - val_loss: 0.7299 - val_accuracy: 0.5375\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7237 - accuracy: 0.5672 - val_loss: 0.7267 - val_accuracy: 0.5375\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7198 - accuracy: 0.5562 - val_loss: 0.7237 - val_accuracy: 0.5375\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7162 - accuracy: 0.5672 - val_loss: 0.7209 - val_accuracy: 0.5125\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7127 - accuracy: 0.5719 - val_loss: 0.7183 - val_accuracy: 0.5250\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7100 - accuracy: 0.5766 - val_loss: 0.7163 - val_accuracy: 0.5312\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7070 - accuracy: 0.5875 - val_loss: 0.7146 - val_accuracy: 0.5250\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.5719 - val_loss: 0.7128 - val_accuracy: 0.5250\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.5766 - val_loss: 0.7111 - val_accuracy: 0.5250\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7007 - accuracy: 0.5734 - val_loss: 0.7101 - val_accuracy: 0.5250\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6987 - accuracy: 0.5719 - val_loss: 0.7085 - val_accuracy: 0.5250\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6971 - accuracy: 0.5750 - val_loss: 0.7073 - val_accuracy: 0.5188\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.6954 - accuracy: 0.5703 - val_loss: 0.7065 - val_accuracy: 0.5312\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.6941 - accuracy: 0.5719 - val_loss: 0.7056 - val_accuracy: 0.5188\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5719 - val_loss: 0.7048 - val_accuracy: 0.5188\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 980us/step - loss: 0.6915 - accuracy: 0.5781 - val_loss: 0.7039 - val_accuracy: 0.5188\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 986us/step - loss: 0.6904 - accuracy: 0.5766 - val_loss: 0.7032 - val_accuracy: 0.5188\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 974us/step - loss: 0.6895 - accuracy: 0.5813 - val_loss: 0.7028 - val_accuracy: 0.5188\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.6887 - accuracy: 0.5781 - val_loss: 0.7021 - val_accuracy: 0.5188\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 1000us/step - loss: 0.6881 - accuracy: 0.5781 - val_loss: 0.7015 - val_accuracy: 0.5312\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.6871 - accuracy: 0.5781 - val_loss: 0.7010 - val_accuracy: 0.5312\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.6863 - accuracy: 0.5734 - val_loss: 0.7004 - val_accuracy: 0.5250\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 965us/step - loss: 0.6855 - accuracy: 0.5781 - val_loss: 0.7002 - val_accuracy: 0.5250\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5766 - val_loss: 0.7004 - val_accuracy: 0.5250\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.6845 - accuracy: 0.5781 - val_loss: 0.6993 - val_accuracy: 0.5188\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 973us/step - loss: 0.6839 - accuracy: 0.5766 - val_loss: 0.6991 - val_accuracy: 0.5312\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 977us/step - loss: 0.6834 - accuracy: 0.5750 - val_loss: 0.6986 - val_accuracy: 0.5188\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 970us/step - loss: 0.6831 - accuracy: 0.5766 - val_loss: 0.6986 - val_accuracy: 0.5250\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6827 - accuracy: 0.5766 - val_loss: 0.6990 - val_accuracy: 0.5312\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5656 - val_loss: 0.6983 - val_accuracy: 0.5312\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6815 - accuracy: 0.5719 - val_loss: 0.6981 - val_accuracy: 0.5250\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.5719 - val_loss: 0.6971 - val_accuracy: 0.5500\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6810 - accuracy: 0.5719 - val_loss: 0.6975 - val_accuracy: 0.5375\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6806 - accuracy: 0.5734 - val_loss: 0.6976 - val_accuracy: 0.5437\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 985us/step - loss: 0.6805 - accuracy: 0.5719 - val_loss: 0.6968 - val_accuracy: 0.5250\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6800 - accuracy: 0.5781 - val_loss: 0.6971 - val_accuracy: 0.5562\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 993us/step - loss: 0.6796 - accuracy: 0.5813 - val_loss: 0.6970 - val_accuracy: 0.5500\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 966us/step - loss: 0.6795 - accuracy: 0.5813 - val_loss: 0.6966 - val_accuracy: 0.5437\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.6792 - accuracy: 0.5781 - val_loss: 0.6969 - val_accuracy: 0.5375\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6791 - accuracy: 0.5859 - val_loss: 0.6961 - val_accuracy: 0.5250\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 970us/step - loss: 0.6786 - accuracy: 0.5781 - val_loss: 0.6964 - val_accuracy: 0.5312\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 974us/step - loss: 0.6785 - accuracy: 0.5781 - val_loss: 0.6965 - val_accuracy: 0.5500\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.6782 - accuracy: 0.5766 - val_loss: 0.6962 - val_accuracy: 0.5562\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.6781 - accuracy: 0.5797 - val_loss: 0.6961 - val_accuracy: 0.5250\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.5781 - val_loss: 0.6973 - val_accuracy: 0.5375\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 973us/step - loss: 0.6775 - accuracy: 0.5734 - val_loss: 0.6963 - val_accuracy: 0.5500\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 966us/step - loss: 0.6773 - accuracy: 0.5781 - val_loss: 0.6956 - val_accuracy: 0.5500\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6774 - accuracy: 0.5891 - val_loss: 0.6954 - val_accuracy: 0.5312\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6775 - accuracy: 0.5859 - val_loss: 0.6957 - val_accuracy: 0.5312\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 967us/step - loss: 0.6774 - accuracy: 0.5750 - val_loss: 0.6959 - val_accuracy: 0.5375\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.6768 - accuracy: 0.5813 - val_loss: 0.6958 - val_accuracy: 0.5375\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 994us/step - loss: 0.6768 - accuracy: 0.5813 - val_loss: 0.6959 - val_accuracy: 0.5437\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.5813 - val_loss: 0.6956 - val_accuracy: 0.5312\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.5922 - val_loss: 0.6954 - val_accuracy: 0.5437\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6762 - accuracy: 0.5859 - val_loss: 0.6957 - val_accuracy: 0.5312\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.5844 - val_loss: 0.6956 - val_accuracy: 0.5312\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.6759 - accuracy: 0.5922 - val_loss: 0.6955 - val_accuracy: 0.5375\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 985us/step - loss: 0.6757 - accuracy: 0.5813 - val_loss: 0.6959 - val_accuracy: 0.5437\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6758 - accuracy: 0.5781 - val_loss: 0.6956 - val_accuracy: 0.5500\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6755 - accuracy: 0.5813 - val_loss: 0.6955 - val_accuracy: 0.5250\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 967us/step - loss: 0.6752 - accuracy: 0.5813 - val_loss: 0.6956 - val_accuracy: 0.5312\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.6754 - accuracy: 0.5844 - val_loss: 0.6955 - val_accuracy: 0.5437\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 978us/step - loss: 0.6752 - accuracy: 0.5938 - val_loss: 0.6949 - val_accuracy: 0.5312\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 972us/step - loss: 0.6749 - accuracy: 0.5859 - val_loss: 0.6954 - val_accuracy: 0.5375\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 970us/step - loss: 0.6749 - accuracy: 0.5906 - val_loss: 0.6953 - val_accuracy: 0.5437\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.6748 - accuracy: 0.5906 - val_loss: 0.6953 - val_accuracy: 0.5375\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 963us/step - loss: 0.6747 - accuracy: 0.5859 - val_loss: 0.6954 - val_accuracy: 0.5437\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 978us/step - loss: 0.6756 - accuracy: 0.5797 - val_loss: 0.6953 - val_accuracy: 0.5312\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 967us/step - loss: 0.6744 - accuracy: 0.5859 - val_loss: 0.6953 - val_accuracy: 0.5437\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.5922 - val_loss: 0.6950 - val_accuracy: 0.5500\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 985us/step - loss: 0.6746 - accuracy: 0.5922 - val_loss: 0.6948 - val_accuracy: 0.5375\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 960us/step - loss: 0.6739 - accuracy: 0.5906 - val_loss: 0.6954 - val_accuracy: 0.5437\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6743 - accuracy: 0.5859 - val_loss: 0.6955 - val_accuracy: 0.5500\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6741 - accuracy: 0.5875 - val_loss: 0.6950 - val_accuracy: 0.5375\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.5906 - val_loss: 0.6950 - val_accuracy: 0.5437\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.5859 - val_loss: 0.6951 - val_accuracy: 0.5312\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.6737 - accuracy: 0.5875 - val_loss: 0.6953 - val_accuracy: 0.5312\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6739 - accuracy: 0.5922 - val_loss: 0.6950 - val_accuracy: 0.5312\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6739 - accuracy: 0.5875 - val_loss: 0.6956 - val_accuracy: 0.5312\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.5953 - val_loss: 0.6949 - val_accuracy: 0.5250\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6734 - accuracy: 0.5859 - val_loss: 0.6952 - val_accuracy: 0.5437\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6732 - accuracy: 0.5906 - val_loss: 0.6953 - val_accuracy: 0.5500\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6734 - accuracy: 0.5906 - val_loss: 0.6951 - val_accuracy: 0.5437\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.6733 - accuracy: 0.5875 - val_loss: 0.6947 - val_accuracy: 0.5312\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.6734 - accuracy: 0.5891 - val_loss: 0.6951 - val_accuracy: 0.5500\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6729 - accuracy: 0.5859 - val_loss: 0.6948 - val_accuracy: 0.5437\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6730 - accuracy: 0.5859 - val_loss: 0.6950 - val_accuracy: 0.5375\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 968us/step - loss: 0.6730 - accuracy: 0.5891 - val_loss: 0.6946 - val_accuracy: 0.5312\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 994us/step - loss: 0.6730 - accuracy: 0.5875 - val_loss: 0.6948 - val_accuracy: 0.5500\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 998us/step - loss: 0.6728 - accuracy: 0.5891 - val_loss: 0.6954 - val_accuracy: 0.5437\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6725 - accuracy: 0.5813 - val_loss: 0.6949 - val_accuracy: 0.5312\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.5906 - val_loss: 0.6948 - val_accuracy: 0.5375\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.6726 - accuracy: 0.5922 - val_loss: 0.6950 - val_accuracy: 0.5250\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6727 - accuracy: 0.5922 - val_loss: 0.6947 - val_accuracy: 0.5375\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 996us/step - loss: 0.6725 - accuracy: 0.5969 - val_loss: 0.6948 - val_accuracy: 0.5437\n",
      "7/7 [==============================] - 0s 499us/step - loss: 0.6792 - accuracy: 0.5800\n",
      "Test Loss: 0.6792, Test Accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0.5, X[:, 1] > 0.5).astype(int)\n",
    "noise = np.random.normal(0, 0.5, size=X.shape)\n",
    "X = X + noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è With a smaller batch size, your model will end up overfitting faster... ‚ùóÔ∏è\n",
    "\n",
    "üëâ Although, let's keep ***`batch size = 1`*** for the next question and try to understand how to prevent overfitting using the strategy of `regularization`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about regularization** ‚ùì \n",
    "\n",
    "Can we ***regularize*** our network to ***avoid overfitting***? \n",
    "\n",
    "* Keep the batch size to 1,\n",
    "* Add a `L2-regularization`,\n",
    "* Increase the power of this L2-regularization until it smooths out the decision boundary! \n",
    "Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "640/640 [==============================] - 1s 710us/step - loss: 1.0277 - accuracy: 0.5188 - val_loss: 0.7448 - val_accuracy: 0.4812\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 0s 448us/step - loss: 0.7094 - accuracy: 0.5297 - val_loss: 0.6957 - val_accuracy: 0.5188\n",
      "Epoch 3/100\n",
      "640/640 [==============================] - 0s 431us/step - loss: 0.6942 - accuracy: 0.5156 - val_loss: 0.6927 - val_accuracy: 0.5188\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 0s 444us/step - loss: 0.6933 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 0s 418us/step - loss: 0.6934 - accuracy: 0.5063 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 0s 428us/step - loss: 0.6933 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 0s 435us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5188\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6926 - val_accuracy: 0.5188\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6932 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 0s 442us/step - loss: 0.6932 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 0s 411us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6933 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 0s 419us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 0s 409us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 0s 411us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 16/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 0s 411us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 19/100\n",
      "640/640 [==============================] - 0s 439us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 0s 411us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 0s 409us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 0s 443us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 0s 441us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 0s 423us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 0s 451us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 0s 423us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 0s 422us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 0s 421us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 0s 417us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 0s 424us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 0s 415us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 0s 450us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 0s 423us/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 0s 420us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 44/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 0s 422us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 0s 425us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 0s 455us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 50/100\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 0s 433us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 0s 427us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 53/100\n",
      "640/640 [==============================] - 0s 429us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 54/100\n",
      "640/640 [==============================] - 0s 417us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 0s 447us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 0s 417us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 0s 412us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 0s 433us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 62/100\n",
      "640/640 [==============================] - 0s 416us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 0s 449us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 0s 444us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 0s 424us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 0s 446us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 67/100\n",
      "640/640 [==============================] - 0s 412us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 0s 416us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 69/100\n",
      "640/640 [==============================] - 0s 430us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 0s 414us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 0s 432us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 0s 426us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 0s 433us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 75/100\n",
      "640/640 [==============================] - 0s 410us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 0s 413us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 0s 440us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 0s 476us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 0s 458us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 0s 434us/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 0s 432us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 83/100\n",
      "640/640 [==============================] - 0s 417us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 0s 460us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 0s 446us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 86/100\n",
      "640/640 [==============================] - 0s 434us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 87/100\n",
      "640/640 [==============================] - 0s 421us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 0s 415us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 0s 431us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 90/100\n",
      "640/640 [==============================] - 0s 479us/step - loss: 0.6930 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 91/100\n",
      "640/640 [==============================] - 0s 442us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 0s 424us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 93/100\n",
      "640/640 [==============================] - 0s 425us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 0s 434us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 0s 460us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 0s 437us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 0s 431us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6924 - val_accuracy: 0.5188\n",
      "Epoch 98/100\n",
      "640/640 [==============================] - 0s 433us/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 0s 511us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 0s 488us/step - loss: 0.6929 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5188\n",
      "7/7 [==============================] - 0s 621us/step - loss: 0.6962 - accuracy: 0.4600\n",
      "Test Loss: 0.6962, Test Accuracy: 0.4600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0.5, X[:, 1] > 0.5).astype(int)\n",
    "noise = np.random.normal(0, 0.5, size=X.shape)\n",
    "X = X + noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=(X_train.shape[1],), \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(8, activation='relu', \n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions with the spiral dataset** ‚ùì \n",
    "\n",
    "<u>Configuration</u>:\n",
    "\n",
    "* Select the `spiral dataset`,\n",
    "* Remove regularization,\n",
    "* Increase the `ratio of training to test data` to 80%. \n",
    "\n",
    "<u>Neural Network</u>: 3 hidden layers with:\n",
    "* 8 neurons on the first layer, \n",
    "* 7 neurons on the second layer,\n",
    "* 6 neurons on the third layer. \n",
    "\n",
    "<u>Experiment</u>:\n",
    "\n",
    "* Run the algorithm with a batch size of 30,\n",
    "* Make sure to run it for at least 1500 epochs,\n",
    "* Then, compare it to the same run but with a batch size of 1. \n",
    "\n",
    "You can check what happens on the train loss and test loss step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Add your comments here</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the <u>`circle dataset`</u>:\n",
    "* with no noise,\n",
    "* and a *ratio of training to test data* of 50%,\n",
    "* Use a batch size of 20. \n",
    "\n",
    "Use a <u>neural network</u> with:\n",
    "* one layer of 5 neurons,\n",
    "* no regularization, \n",
    "* and the tanh activation function\n",
    "\n",
    "‚ùì **Question about the learning rate** ‚ùì \n",
    "\n",
    "For each learning rate (from 0.0001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è When you change the learning rate, make sure to reinitialize the neural network (_circular arrow, left to the play/pause button_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "test_loss = [0.1, 0.08, 0.05, 0.02, 0.1, 0.5]\n",
    "\n",
    "plt.plot(np.log(learning_rates), test_loss, marker='o')\n",
    "plt.title('Test Loss vs. Log Learning Rate')\n",
    "plt.xlabel('Log Learning Rate')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è Too low and too high learning rates both lead to a high test loss... but not for the same reasons!\n",
    "\n",
    "* A **low learning rate** helps a neural network converge in a similar fashion to a moderate learning rate but... way slower... i.e. more epochs would be needed!\n",
    "* A **high learning rate** makes the algorithm diverge completely.\n",
    "    - Try a learning rate $ \\alpha = 10 $ with 400 epochs, you should see the loss vary. This corresponds to the fact that the algorithms converge to *different local minima*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
